---
title: "Assignment #3: Basic text pipeline proficiency"
author: "Taufiq Ahmad"
date: "11-05-2024"
output: html_document
---
# Focusing on the requirement of this assignment 

For this assignment, you will create an R markdown document, import a text dataset

Use this PDF of 32 articles about journalist and political operative Raymond MoleyLinks to an external site.

Load the appropriate software libraries

Import the data and compile the articles into a dataframe, one row per sentence.

Then tokenize the data, one word per row

Clean the data

Generate a list of the top 20 bigrams

Create a ggplot chart showing the top 20 bigrams

At the bottom of the R markdown document, write a 300 word memo describing your key findings.

You are welcome to consult your notes, books, the internet, AI tools or work in groups. Make sure you hand in your own work.

Hand in a link to your personal GitHub repository in Elms that contains the data and your code. Make sure the code links to your GitHub and not to your personal computer's hard drive.


# Scraping ther PDF 
# spliting into separate text files

```{r}
#install.packages("pdftools")
library(tidyverse)
library(pdftools)
library(stringr)
library(tidytext)
```

## Converting PDF files to text
##Download PDF from Rob's Github page. 

```{r}
#using the url of online file to download. 

url <- "https://github.com/wellsdata/CompText_Jour/blob/main/exercises/assets/pdfs/moley_news.PDF?raw=true"

destfile <- "assets/pdfs/moley_text.pdf"

download.file(url, destfile, mode = "wb")

text <- pdf_text("assets/pdfs/moley_text.pdf")

writeLines(text, "assets/extracted_text/moley_text.txt")

```

## Createed an index from the first extracted page
```{r}
moley_index <- read_lines("../exercises/assets/extracted_text/moley_text.txt")
extracted_lines <- moley_index[16:173]

extracted_lines <- extracted_lines |> 
  as.data.frame() |> 
  mutate(extracted_lines = str_replace_all(extracted_lines, "\\s*\\|\\s*About LexisNexis\\s*\\|\\s*Privacy Policy\\s*\\|\\s*Terms & Conditions\\s*\\|\\s*Copyright Â© 2020 LexisNexis\\s*\\|?\\s*", "")) 

extracted_lines
```

## Clean, Split text to separate articles on common identifier

```{r}

file_path <- "../exercises/assets/extracted_text/moley_text.txt"
text_data <- readLines(file_path)

text_combined <- paste(text_data, collapse = "\n")

documents <- strsplit(text_combined, "End of Document")[[1]]

documents <- lapply(documents, function(doc) {
  doc <- str_replace_all(doc, "Classification[\\s\\S]*?Load-Date:.*?\\n", "")
  
  doc <- str_replace_all(doc, "(Section:|Length:|Body)\\s*.*?(\\r?\\n){2,}", "")
  
  return(doc)
})

documents <- unlist(documents)
output_dir <- "../exercises/assets/extracted_text/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("moley_extracted_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}

cat("Files created:", length(documents), "\n")
```

```{r}
#here is the index from first article
article1 <- read_lines("../exercises/assets/extracted_text/moley_extracted_1.txt")
# Extract lines with only the text
article1 <- article1[184:217]

file1 <- "../exercises/assets/extracted_text/moley_extracted_1.txt"
writeLines(article1, file1)

```

## Build a final dataframe index

```{r}
# Triming the spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
  mutate(
    trimmed_line = str_trim(extracted_lines),  
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )
cleaned_data

cleaned_data <- cleaned_data |>
  mutate(
    is_title = ifelse(lead(is_date, 1), 
                      TRUE, 
                      str_detect(trimmed_line, "^\\d+\\."))
  )
cleaned_data

#Shifting dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), ifelse(lead(is_date, 2), lead(trimmed_line,2), NA_character_)),
      
    trimmed_line = ifelse(is_title,
                   ifelse(lead(is_title, 1),
                          paste0(trimmed_line, lead(trimmed_line, 1), " "),
                          trimmed_line
                          ), 
                   trimmed_line
                   )
    
   )|> 
  filter(is_title) |>
  filter(str_detect(trimmed_line, "^\\d+\\.")) |>
  
  select(trimmed_line, date)

aligned_data

final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )
final_data

final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")

final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

#write_csv(final_data, "./final_data.csv")
```


# Part 2: Compile Text into a Dataframe

## Raw text compiler 
```{r}

files <- list.files("../exercises/assets/extracted_text", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
 mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
  mutate(filepath = paste0("../exercises/assets/extracted_text/", filename))
head(final_index)
```

### Text compiler
```{r}

# Define=ing the function to loop through each text file 
create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

# Create empty tibble to store results
articles_df <- tibble()

row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#write.csv(articles_df, "../exercises/assets/extracted_text/moley_df2.csv")

```

# Bigrams 

#Remove stopwords
```{r}
data(stop_words)
```


# Bigrams

```{r}
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(sentence = str_squish(sentence)) |> 
  mutate(sentence = tolower(sentence)) |>  
  mutate(sentence = str_replace_all(sentence, "title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\.", "")) |> 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

bigrams
```

## Rerun with cleaning of random words from bigrams
```{r}
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(sentence = str_squish(sentence)) |> 
  mutate(sentence = tolower(sentence)) |>  
  mutate(sentence = str_replace_all(sentence, "new york times| news service|rights reserved|authoritative content|wwwalt|newstex|wwwjstororg|enwikipediaorg|wiki|length|words|title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\.", "")) |> 
  mutate(sentence = str_replace_all(sentence, "https?://\\S+", "")) |> 
  mutate(sentence = str_replace_all(sentence, "wwwalt\\S+", "")) |> 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

bigrams
```


#top 20 bigrams
```{r}

top_20_bigrams <- bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
  
```



```{r}
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases in Raymond Moley articles",
       caption = "n=32 articles. Graphic by Taufiq. 11-05-2024",
       x = "Phrase",
       y = "Count of terms")
```
