---
title: "Week_11_homework_Basic text pipeline proficiency"
author: "Taufiq"
date: "2024-11-06"
output: html_document
---

```{r}
#install.packages("pdftools")
library(tidyverse)
library(pdftools)
```

## Convert PDF to text

```{r}
# Using pdftools package. Good for basic PDF extraction

text <- pdf_text("moley_news.PDF")

# pdf_text reads the text from a PDF file.
writeLines(text, "moley_news.text")

# writeLines writes this text to a text file

```

## Split text to separate articles on common identifier

In this case, NexisUni makes life easy for us. At the end of each document, there are the words "End of Document". Convenient! We search for "End of Document" and then instruct R to split the file and dump it into a standalone text file.
```{r}
# Step 1: Read the entire text file into R
#You will need to alter this for your computer
#For Mac: In Finder, Cntl + click on the filename, NOW hold down Alt/Option, and an item to copy file path will appear as Copy "Filename" as Pathname 
#https://stackoverflow.com/questions/52695546/how-to-copy-path-of-a-file-in-mac-os

file_path <- "moley_news.text"
text_data <- readLines(file_path)
```

```{r}
# Step 2: Combine lines into one single string
text_combined <- paste(text_data, collapse = "\n")

# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "End of Document")[[1]]

```

```{r}
# Step 4: Write each section to a new file

output_dir <- "./Extracted"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("moley_extracted", i, ".txt"))
  writeLines(documents[[i]], output_file) 
}

cat("Files created:", length(documents), "\n")
```

## Create an index from the first extracted page
--We are just grabbing the index of the 10 listed items and creating a dataframe
```{r}
Moley_index <- read_lines("./Extracted/moley_extracted1.txt")

# Extract lines 16 to 58
extracted_lines <-Moley_index[6:176]

# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")

extracted_lines <- extracted_lines |> 
  as.data.frame() 
```


## Build a final dataframe index

```{r}
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
  mutate(
    # Trim leading and trailing spaces before detection
    trimmed_line = str_trim(extracted_lines),  

    # Detect titles (start with a number and a period)
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    # Detect dates (e.g., "Aug 14, 2024")
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)  # Keep only the relevant columns

# Step 3: Rename columns for clarity
final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")


#Step 5: Format date, clean headline
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

write_csv(final_data, "./final_data.csv")
  
```

## Raw text compiler 
```{r include=FALSE}
#This creates an index with the file path to the stories. And then it compiles the stories into a dataframe
#####################
# Begin SM Code #####
#####################

###
# List out text files that match pattern .txt, create DF
###

files <- list.files("/Users/taufiqahmad/Desktop/Code/CompText_Jour/exercises/Week_11/Extracted/", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
 mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

#Join the file list to the index

#load final data if you haven't already
# final_data <- read.csv("Week_11/final_data.csv")

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
#you need the actual hard-coded path on this line below to the text
  
  # mutate(filepath = paste0("/Users/taufiqahmad/Desktop/Code/CompText_Jour/exercises/Week_11/Extracted", filename))
mutate(filepath = paste0("./Extracted/", filename))

head(final_index)
```


## Text compiler
```{r}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#After viewing articles_df, I see 64 lines from the index that I don't need. Cutting them 

articles_df <- articles_df %>%
  slice(-c(1:158)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

#write.csv(./final_data.csv)
```


```{r}
# Load required libraries
library(dplyr)
library(stringr)
library(tidytext)


bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      # Remove extra spaces
    sentence = tolower(sentence),
    sentence = str_replace_all(sentence, c(
    "copyright" = "",
    "new york times"="",
    "publication"="",
    "www.alt"="",
    "http"=""))) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1) & !is.na(word2))   
    
# Ensure `stop_words` data is available
data("stop_words")

# Define the expanded remove pattern for unwanted terms
remove_pattern <- paste(
  "title|pages|publication date|publication subject|publication type|issn|language of publication: english|",
  "document url|copyright|last updated|database|startofarticle|proquest document id|",
  "classification|https|--|people|alt|language|english|length|words|publication|type|morg|york|times|'new york times'|publication   info|illustration|date|caption|[0-9.]|","new york times", "identifier/keyword|twitter\\.|rauchway|keynes's|_ftn|enwikipediaorg|","wwwnytimescom|wwwoenbat|wwwpresidencyucsbedu|wwwalt|wwwthemoneyillusioncom|","aaa|predated|a_woman_to_reckon_with_the_vision_and_legacy_of_fran|ab_se|",
  "jcr:fec|ac|___________________|\\bwww\\b|[_]+",sep = ""
)

# Process bigrams
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      # Remove extra spaces
    sentence = tolower(sentence),                         # Convert to lowercase
    sentence = str_replace_all(sentence, remove_pattern, ""), # Remove unwanted terms
    sentence = str_replace_all(sentence, "- ", ""),       # Remove trailing hyphens
    sentence = str_replace_all(sentence, "\\b[a-zA-Z]\\b", "") # Remove single characters
  ) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% remove_pattern) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1) & !is.na(word2))                   # Filter out NAs

bigrams
```

#top 20 bigrams

```{r}
top_20_bigrams <- bigrams %>% 
   slice_max(n, n = 20) %>% 
   mutate(bigram = paste(word1, " ", word2)) %>% 
   select(bigram, n)

top_20_bigrams
```
#Create a ggplot chart showing the top 20 bigrams

```{r}
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Raymond Moley",
       caption = "n=33 articles. Graphic by Taufiq Ahmad 11-05-2024",
       x = "Phrase",
       y = "Count of terms")
```


#At the bottom of the R markdown document, write a 300 word memo describing your key findings.

In this Week 11 Homework assignment, I learned several key techniques and developed a structured approach to achieving accurate results in my work.

First, I imported the necessary libraries  to handle PDF and text manipulation tasks. The pdf_text() function from the pdftools package was used to read textual data from a PDF file, and then written to a .text file for further processing that converted  unstructured PDF data into a workable text format in R.

Next, the text was segmented by identifying the "End of Document" marker which allowed each article to be isolated, making it easier to analyze individually. The structured data is then compiled into a dataframe, with each row representing a distinct sentence. This enables refined sentence-level analysis and sets the stage for more advanced analysis techniques.

For further analyze the data, the text is tokenized into individual words. Unnecessary characters, stop words, and irrelevant elements were removed to improve data clarity and focus. After cleaning, bigrams were generated to examine common phrases to get insight into frequently paired words. The las part of this assignment provide top 20 bigrams from the text, presenting frequent word pairings and key patterns within the dataset.

This assignment has given me a valuable experience in handling and preparing textual data for PDF text analysis and insights. It has equipped me with foundational skills in text preprocessing, which are essential for computational text analysis. Through repetitive practice in tokenizing data, cleaning text, generating bigram lists, and creating ggplot visualizations, I have gained confidence in applying these techniques to future projects.